---
title: "UD6 N01"
output:
  pdf_document:
    latex_engine: xelatex
  html_notebook: default
  html_document: default
---

# GLMs

Ya hemos visto los casos binomial y gaussiano, atenderemos a los casos multinomial, gamma y binomial negativa.

## GlM Binomial negativa: sirve para variables de conteo

Cargamos el dataset de cangrejos con satélites

      Each female horseshoe crab in the study had a male crab attached to her in her nest.
      The study investigated factors that affect whether the female crab had any other males, called satellites, residing near her. 
      Explanatory variables that are thought to affect this included the female crab’s color (C), spine condition (S), weight (Wt), 
      and carapace width (W). 
      The response outcome for each female crab is her number of satellites (Sa). There are 173 females in this study
      
Se quiere explicar el número de satélites del cangrejo en función de los atributos descritos, como es una variable de conteo al ser un entero no negativo, el **glm binomial negativo** es el que mejor se adapta
```{r}
url <- 'https://onlinecourses.science.psu.edu/stat504/sites/onlinecourses.science.psu.edu.stat504/files/lesson07/crab.txt'
df <- read.csv(url,header= F,sep = '\t')
write.csv(df, 'crabs.csv')
crab <- read.table(url)
colnames(crab) <- c("Obs","C","S","W","Wt","Sa")
head(crab)
summary(crab)
crab <- crab[,-1]
dim(crab)
crab$C <- as.factor(crab$C)
crab$S <- as.factor(crab$S)
head(crab)
```

Entrenamos el modelo que supone que todos los cangrejos tienen los mismos satélites
```{r}
library(MASS)
model <- glm.nb(Sa~1, data = crab)
summary(model)
model$fitted
model$coefficients
```



Hacemos un modelo sobre el peso y observamos si es significativo
```{r}
model <- glm.nb(Sa~W, data = crab)
summary(model)
```




Observamos los valores predichos junto a su valor real
```{r}
head(data.frame(crab,pred=model$fitted),20)
```

Representamos el peso y la cantidad de satélites y observamos que tiene poca capacidad predictiva
```{r}
plot(crab$W,crab$Sa)
```



Hacemos una predicción
```{r}
newdt <- data.frame(W=26.3)
predict(model, newdt,type="response")
```



Averiguamos cuál es el mejor modelo para los datos usando el paso AIC
```{r}
fit1 <- glm.nb(Sa~., data=crab)
fit0 <- glm.nb(Sa~1, data=crab)
library(MASS)
step <-stepAIC(fit0,direction="forward",scope=list(upper=fit1,lower=fit0))
step
summary(step)
```

Entrenamos el modelo glm binomial negativa con los valores óptimos y observamos los *odds ratios*: aumentar una unidad el peso Wt duplica los satélites, poseer cada una de las 3 categorías seleccionadas del factor C aporta ratios negativos (disminuye la cantidad de satélites)
```{r}
mod <- glm.nb(formula = Sa ~ Wt + C, data = crab, init.theta = 0.9556206187, 
              link = log)

exp(mod$coefficients)
```

Vemos los valores mejor ajustados
```{r}
data.frame(crab,pred=mod$fitted)
```


Los coeficientes y su intervalo de confianza son:
```{r}
confint(mod)
```



## GLM Gamma: regresión Gamma
La distribución Gamma sirve para hacer regresión sobre datos que toman positivos y están sesgados hacia la derecha. 
Funciona bien y puede aproximar distribuciones como la lognormal sin problema dada su flexibilidad. Un ejemplo de este tipo de datos
suele ser el ROI en forma de ratio.

Hay dos tipos de función link posibles para la gamma: "identity", "log" e "inverse".

Vemos un ejemplo de regresión Gamma

```{r}
# install.packages("faraway")
library(faraway)
data(wafer)
help(wafer)
head(wafer)
#observamos que toma valores positivos la variable objetivo
plot(density(wafer$resist))
summary(wafer)
```



Lo habitual para hacer regresión sobre una distirbución sesgada hacia la derecha es tomar logaritmos:
```{r}
res.lm.logY <- lm(log(resist) ~ x1 + x2 + x3 + x4, data = wafer)
summary(res.lm.logY)
```


Probamos un modelo con glm gamma para ver que son equivalentes:
```{r}
res.glm.Gamma.log <- glm(formula = resist ~ x1 + x2 + x3 + x4,
                         family  = Gamma(link = "log"),
                         data    = wafer)
summary(res.glm.Gamma.log)
```



Pintamos los residuos y vemos que son iguales
```{r}
hist(res.lm.logY$residuals)
hist(res.glm.Gamma.log$residuals)
```



Probamos la versión con link "identity", tiene mayor AIC
```{r}
res.glm.Gamma.identity <- glm(formula = resist ~ x1 + x2 + x3 + x4,
                              family  = Gamma(link = "identity"),
                              data    = wafer)
summary(res.glm.Gamma.identity)
hist(res.glm.Gamma.identity$residuals)
```



Probamos la versión con link "inverse"", tiene menor AIC que las dos anteriores, parece preferible.

```{r}
res.glm.Gamma.inverse <- glm(formula = resist ~ x1 + x2 + x3 + x4,
                              family  = Gamma(link = "inverse"),
                              data    = wafer)
summary(res.glm.Gamma.inverse)
hist(res.glm.Gamma.inverse$residuals)
```



Buscamos el modelo Gamma mejor dentro de las posibilidades usando el criterio AIC
```{r}
fit1 <- glm(resist~., data=wafer, family=Gamma(link = "inverse"))
fit0 <- glm(resist~1, data=wafer, family=Gamma(link = "inverse"))
library(MASS)
step <-stepAIC(fit0,direction="forward",scope=list(upper=fit1,lower=fit0))
step$coefficients
summary(step)
```







## GLM Multinomial: clasificación múltiple

El GLM Multinomial se aplica cuando tenemos que realizar una clasificación con 3 categorías o más. Cargamos el dataset iris, que tiene atributos de 3 flores y una columna con el valor de la clase.
```{r}
# install.packages(c("glmnet","ISLR"))
library(glmnet,help=T)
help("glmnet")
library(glmnet)
library("foreign")
data(iris)
dim(iris)
head(iris)
str(iris)
```


Como "glmnet" no acepta factores, transformamos la columna Species en categorías numéricas
```{r}
iris$Species <- as.numeric(iris$Species)
unique(iris$Species)

X <- as.matrix(subset(iris, select=- Species ))
y <- iris$Species
head(X)
```
Ajustamos el modelo multinomial
```{r}
mod <- glmnet(X, y, alpha=0, family = "multinomial",type.multinomial="ungrouped")
summary(mod)
```

```{r}
print(mod)
plot(mod,xvar = "lambda", label = TRUE)
```


Damos los coeficientes según el valor de regularización:
```{r}
coef(mod,s=0)
coef(mod,s=0.1)
coef(mod,s=0.13)
coef(mod,s=0.29)
```

Obtenemos las probabilidades de cada valor
```{r}
predict(mod,newx = X[1:2,],s=0,type = "response")
summary(mod)
```



Calculamos el mejor modelo posible (parecido a stepAIC)
```{r}
cvfit <- cv.glmnet(X, y, alpha=0, family = "multinomial",type.multinomial="ungrouped")
plot(cvfit)
coef(cvfit, s = "lambda.min")
```





