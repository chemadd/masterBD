{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Cargamos el corpus\n",
      "['Me gustan las vacas', 'Me gustan los caballos', 'odio los perros', 'odio los caballos', 'me gustan las ranas', 'me gusta el helado', 'no quiero comer', 'los helados son cremosos']\n",
      "\n",
      "\n",
      "2. Vectorizamos los textos\n",
      "\n",
      "Atributos: ['caballos', 'comer', 'cremosos', 'gusta', 'gustan', 'helado', 'helados', 'odio', 'perros', 'quiero', 'ranas', 'vacas']\n",
      "\n",
      "Matriz tf:\n",
      " [[0 0 0 0 1 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 0]]\n",
      "\n",
      "nº de muestras: 8, nº de atributos: 12\n",
      "\n",
      "Topic 0:\n",
      "  caballos\n",
      "  gustan\n",
      "  cremosos\n",
      "Me gustan los caballos\n",
      "los helados son cremosos\n",
      "me gustan las ranas\n",
      "\n",
      "Topic 1:\n",
      "  gustan\n",
      "  quiero\n",
      "  comer\n",
      "no quiero comer\n",
      "Me gustan las vacas\n",
      "me gustan las ranas\n",
      "\n",
      "Topic 2:\n",
      "  odio\n",
      "  helado\n",
      "  gusta\n",
      "me gusta el helado\n",
      "odio los perros\n",
      "odio los caballos\n"
     ]
    }
   ],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#***************************************************************************************\n",
    "# 1. Cargamos el corpus de textos\n",
    "#***************************************************************************************\n",
    "\n",
    "print(\"\\n\\n1. Cargamos el corpus\")\n",
    "trainCorpus = [\"Me gustan las vacas\",\n",
    "               \"Me gustan los caballos\",\n",
    "               \"odio los perros\",\n",
    "               \"odio los caballos\",\n",
    "               \"me gustan las ranas\",\n",
    "               \"me gusta el helado\",\n",
    "               \"no quiero comer\",\n",
    "               \"los helados son cremosos\"]\n",
    "print (trainCorpus)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#*********************************************************************************************************\n",
    "# 2. Vectorizamos los textos del corpus (convertimos cada texto en un vector de frecuencias de palabras)\n",
    "#*********************************************************************************************************\n",
    "print(\"\\n\\n2. Vectorizamos los textos\")\n",
    "\n",
    "puncts = [c for c in string.punctuation]\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')+puncts\n",
    "vectorizer = CountVectorizer(stop_words=spanish_stopwords)\n",
    "\n",
    "#Transformamos los documentos en una matriz de tf's de documentos.\n",
    "vectorizer.fit(trainCorpus) #El vectorizador aprende el vocabulario del corpus\n",
    "print (\"\\nAtributos:\",vectorizer.get_feature_names())\n",
    "tfMatrix = vectorizer.transform(trainCorpus) #Extraemos las frecuencias de palabras (tf)\n",
    "print (\"\\nMatriz tf:\\n\",tfMatrix.toarray())\n",
    "\n",
    "\n",
    "#La matriz tf es nuestro dataset, donde:\n",
    "# - cada fila representa una muestra (un documento del corpus)\n",
    "# - cada columna representa un atributo (la frecuencia de una palabra en dicho documento)\n",
    "print(\"\\nnº de muestras: %d, nº de atributos: %d\" % tfMatrix.shape)\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#*********************************************************************************************************\n",
    "# 3. Aplicamos el algoritmo LDA para extraer los topics\n",
    "#*********************************************************************************************************\n",
    "topics = 3\n",
    "lda_model = LatentDirichletAllocation(n_components =topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tfMatrix)\n",
    "H = lda_model.components_\n",
    "W = lda_model.transform(tfMatrix)\n",
    "\n",
    "no_top_words = 3\n",
    "no_top_documents = 3\n",
    "\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"\\nTopic %d:\" % (topic_idx))\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            print(\" \",feature_names[i])\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(trainCorpus[doc_index])\n",
    "\n",
    "display_topics(H, W, tf_feature_names, trainCorpus, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
