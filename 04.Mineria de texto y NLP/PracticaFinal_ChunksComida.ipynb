{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es programar una función que reciba como input un texto de usuario y devuelva los fragmentos de texto (chunks) que hagan referencia a las comidas y cantidades que ha solicitado. No es necesario, ni es el objetivo de este ejercicio, construir un clasificador de intención previo a esta función, sino simplemente una función que presuponemos recibe una frase con la intención 'Pedir_comida'. Tampoco es objetivo normalizar la salida (por ej.: no es necesario convertir 'tres' a '3' ni 'pizzas' a 'pizza'). Es, por tanto, un ejercicio de mínimos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el alumno deberá usar un NaiveBayesClassifier, en lugar del MaxEntClassifier, para localizar los elementos descritos anteriormente (comida y cantidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deberá comenzar la práctica por el nivel más básico de dificultad (RegexParser) y, en caso de conseguirlo, añadir los siguientes niveles de forma sucesiva. De esta forma, el entregable contendrá todas y cada una de las tres formas de solucionar el problema. No basta, por tanto, con incluir, por ejemplo, únicamente un NaiveBayesClassifier, hay que incluir también las otras dos formas si se quiere obtener la máxima puntuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para llevar a cabo la práctica, deberá construirse una cadena NLP con NLTK, con los siguientes elementos:\n",
    "\n",
    "    segmentación de frases,\n",
    "    tokenización,\n",
    "    POS tagger (analizador mofológico para el español).\n",
    "\n",
    "A continuación, los POS tags obtenidos serán usados por el RegexParser, el UnigramParser, el BigramParser y el NaiveBayesClassifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importamos las librerias necesarias\n",
    "from nltk import Tree\n",
    "from nltk.chunk import *\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk import UnigramTagger, BigramTagger, TrigramTagger, DefaultTagger\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando sentencias de aprendizaje... 2018-09-03 21:42:47.562698\n",
      "Entrenando al Tagger ... 2018-09-03 21:43:16.083758\n",
      "Tagger entrenado... 2018-09-03 21:43:18.465931\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos el tagger\n",
    "\n",
    "# Procedemos al entrenamiento utilizando los tags y corpus de cess_esp\n",
    "sents = cess_esp.tagged_sents()\n",
    "#Metemos en el conjunto de entrenamiento el 90% de las frases, y el restante 10% en el conjunto de test\n",
    "training = []\n",
    "test = []\n",
    "print(\"Generando sentencias de aprendizaje...\", datetime.datetime.now())\n",
    "for i in range(len(sents)):\n",
    "    if i % 10:\n",
    "        training.append(sents[i])\n",
    "    else:\n",
    "        test.append(sents[i])\n",
    "\n",
    "# Establecemos la cadena de taggeo, utilizando el HMM en primer lugar y haciendo que,\n",
    "# si no reconoce una palabra la intente taggear usando otro tagger y asi sucesivamente.\n",
    "print(\"Entrenando al Tagger ...\", datetime.datetime.now())\n",
    "default_tagger = DefaultTagger ('DEFAULT')\n",
    "unigram_tagger = UnigramTagger(training, backoff=default_tagger)\n",
    "bigram_tagger = BigramTagger(training, backoff=unigram_tagger)\n",
    "print(\"Tagger entrenado...\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk.RegexpParser with 3 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "    Comida   <ChunkRule: '(<CANT> <TipoCOMIDA | INGR>)'>\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<Comida> (<INGR>+|(<INGR>*<.*><INGR>)+)'>\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<TipoCOMIDA>| <INGR>'>\n"
     ]
    }
   ],
   "source": [
    "# Una vez entrenado procedemos a analizar las frases de nuestro corpus mediante un \n",
    "# Regexp parser para obtener las entidades que aparezcan en los textos\n",
    "# Definimos la gramatica que nos permita reconocer comidas y cantidades\n",
    "# Definimos un pos-tagger con RegExp para calificar cantidades, comidas e ingredientes\n",
    "patterns = [\n",
    "    (r'([0-9]+|unas|una|un|unos)','CANT'),\n",
    "    (r'bocadillo|bocata|pizza|piza|pincho|racion|plato', 'TipoCOMIDA'),\n",
    "    (r'jamon|chorizo|queso|calamares|pimientos|patatas|tortilla','INGR')\n",
    "]\n",
    "regexp_tagger = nltk.RegexpTagger(patterns, backoff=bigram_tagger)\n",
    "grammarComida = r\"\"\"\n",
    "  Comida: {(<CANT> <TipoCOMIDA | INGR>)} # Comida\n",
    "  ComidaElaborada: {<Comida> (<INGR>+|(<INGR>*<.*><INGR>)+)}\n",
    "  ComidaIndefinida: {<TipoCOMIDA>| <INGR>}\n",
    "  \"\"\"\n",
    "regex_parser = nltk.RegexpParser(grammarComida)\n",
    "print(regex_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion de extraccion de informacion de pedidos. Recibe como parametro un texto con\n",
    "# la solicitud del cliente y devuelve el pedido (alimento y cantidad), si es que lo hay\n",
    "def extract_pedidos (solicitud):\n",
    "    # Tokenizamos la frase\n",
    "    tokens = nltk.word_tokenize(solicitud)\n",
    "    # Asociamos los tags propios de la gramatica que identifica comidas\n",
    "    taggeado = regexp_tagger.tag(tokens)\n",
    "    # Parseamos la frase para obtener el arbol sintactico\n",
    "    arbol = regex_parser.parse(taggeado) # (taggeado, trace = True)\n",
    "    # Convertimos en estructura de arbol, que mantiene relacion con su predecesor:\n",
    "    parentTree = nltk.tree.ParentedTree.convert(arbol)\n",
    "    # Recorremos el arbol buscando el contexto de cada tag, para quedarnos con los\n",
    "    # pedidos completos que haya dentro de cada solicitud\n",
    "    comidas = []\n",
    "    for subarbol in parentTree.subtrees(filter=lambda t: t.label()=='ComidaElaborada'\n",
    "                                       or t.label()=='Comida' or t.label()=='ComidaIndefinida'):\n",
    "        # Si el padre del subarbol sintactico es S, examinamos el subarbol \n",
    "        # ya que contiene un pedido. Si el padre no es S, es la raiz Comida de ComidaElaborada\n",
    "        pedidoParcial = {}\n",
    "        if subarbol.parent().label() == 'S':\n",
    "            # Vamos rellenando los distintos elementos del pedido\n",
    "            ingredientes = []\n",
    "            for hoja in subarbol.leaves():\n",
    "                if hoja[1] == 'CANT':\n",
    "                    pedidoParcial['cantidad'] = hoja[0]\n",
    "                elif hoja[1] == 'INGR':\n",
    "                    ingredientes.append(hoja[0])\n",
    "                elif hoja[1] == 'TipoCOMIDA':\n",
    "                    pedidoParcial['tipoComida'] = hoja[0]\n",
    "            pedidoParcial['ingredientes'] = ingredientes\n",
    "            # Buscamos si hay cantidad y si no es asi, damos valor por defecto:\n",
    "            if 'cantidad' not in pedidoParcial:\n",
    "                pedidoParcial['cantidad'] = 1\n",
    "        comidas.append(pedidoParcial)\n",
    "    # Imprimimos el resultado    \n",
    "    print(comidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cantidad': 'un', 'ingredientes': ['jamon']}, {'cantidad': 'un', 'tipoComida': 'plato', 'ingredientes': ['calamares', 'pimientos', 'patatas']}, {}]\n"
     ]
    }
   ],
   "source": [
    "# Prueba 1\n",
    "sentencia1 = \"Quiero un jamon y un plato de calamares, pimientos y patatas\"\n",
    "extract_pedidos(sentencia1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cantidad': 'un', 'ingredientes': ['jamon']}, {'cantidad': 'una', 'tipoComida': 'piza', 'ingredientes': ['chorizo', 'queso']}, {}, {'cantidad': '5', 'tipoComida': 'bocadillo', 'ingredientes': ['tortilla']}, {}]\n"
     ]
    }
   ],
   "source": [
    "# Prueba 2\n",
    "sentencia2 = \"Quiero un jamon y una piza de chorizo, queso y 5 bocadillo de tortilla\"\n",
    "extract_pedidos(sentencia2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cantidad': 'un', 'ingredientes': ['jamon']}, {'tipoComida': 'pizza', 'ingredientes': [], 'cantidad': 1}]\n"
     ]
    }
   ],
   "source": [
    "# Prueba 3\n",
    "sentencia3 = \"Quiero un jamon y pizza\"\n",
    "extract_pedidos(sentencia3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
