{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es programar una función que reciba como input un texto de usuario y devuelva los fragmentos de texto (chunks) que hagan referencia a las comidas y cantidades que ha solicitado. No es necesario, ni es el objetivo de este ejercicio, construir un clasificador de intención previo a esta función, sino simplemente una función que presuponemos recibe una frase con la intención 'Pedir_comida'. Tampoco es objetivo normalizar la salida (por ej.: no es necesario convertir 'tres' a '3' ni 'pizzas' a 'pizza'). Es, por tanto, un ejercicio de mínimos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el alumno deberá usar un NaiveBayesClassifier, en lugar del MaxEntClassifier, para localizar los elementos descritos anteriormente (comida y cantidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deberá comenzar la práctica por el nivel más básico de dificultad (RegexParser) y, en caso de conseguirlo, añadir los siguientes niveles de forma sucesiva. De esta forma, el entregable contendrá todas y cada una de las tres formas de solucionar el problema. No basta, por tanto, con incluir, por ejemplo, únicamente un NaiveBayesClassifier, hay que incluir también las otras dos formas si se quiere obtener la máxima puntuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para llevar a cabo la práctica, deberá construirse una cadena NLP con NLTK, con los siguientes elementos:\n",
    "\n",
    "    segmentación de frases,\n",
    "    tokenización,\n",
    "    POS tagger (analizador mofológico para el español).\n",
    "\n",
    "A continuación, los POS tags obtenidos serán usados por el RegexParser, el UnigramParser, el BigramParser y el NaiveBayesClassifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creamos un corpus de ejemplo para el reconocimiento de peticiones de comida:\n",
    "corpus_comida = [\"Me gustaria tomar unas rabas\",\n",
    "                 \"Quisiera un pincho de tortilla\", \n",
    "                 \"Me encantaria unas gambas a la plancha\",\n",
    "                 \"Me pones un bocadillo de bacon con queso?\",\n",
    "                 \"Podrias ponerme una ensalada de tomate?\",\n",
    "                 \"Un bocata de lomo\",\n",
    "                 \"Unos pimientos del piquillo\",\n",
    "                 \"Me puedes traer un plato combinado?\",\n",
    "                 \"Podria ser un filete poco hecho?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando sentencias de aprendizaje... 2018-08-31 19:01:44.380874\n",
      "Entrenando al Tagger ... 2018-08-31 19:02:14.599688\n",
      "Tagger entrenado... 2018-08-31 19:02:19.593136\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "# Entrenamos el tagger\n",
    "from nltk.corpus import cess_esp\n",
    "\n",
    "from nltk import UnigramTagger, BigramTagger, TrigramTagger,DefaultTagger\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "# Procedemos al entrenamiento utilizando los tags de cess_esp\n",
    "sents = cess_esp.tagged_sents()\n",
    "#Metemos en el conjunto de entrenamiento el 90% de las frases, y el restante 10% en el conjunto de test\n",
    "training = []\n",
    "test = []\n",
    "print(\"Generando sentencias de aprendizaje...\", datetime.datetime.now())\n",
    "for i in range(len(sents)):\n",
    "    if i % 10:\n",
    "        training.append(sents[i])\n",
    "    else:\n",
    "        test.append(sents[i])\n",
    "\n",
    "# Establecemos la cadena de taggeo, utilizando el HMM en primer lugar y haciendo que,\n",
    "# si no reconoce una palabra la intente taggear usando otro tagger y asi sucesivamente.\n",
    "print(\"Entrenando al Tagger ...\", datetime.datetime.now())\n",
    "default_tagger = DefaultTagger ('NLTK_FASHION')\n",
    "unigram_tagger = UnigramTagger(training, backoff=default_tagger)\n",
    "bigram_tagger = BigramTagger(training, backoff=unigram_tagger)\n",
    "trigram_tagger = TrigramTagger(training, backoff=bigram_tagger)\n",
    "hmm_tagger = HiddenMarkovModelTagger.train(training)\n",
    "print(\"Tagger entrenado...\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk.RegexpParser with 3 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "    Comida   <ChunkRule: '<bocadillo|bocata|pizza|piza|pincho|racion|plato>'>\n",
      "RegexpChunkParser with 1 rules:\n",
      "    Ingredientes   <ChunkRule: '(jamon|chorizo|queso|calamares|pimientos|patatas|tortilla)'>\n",
      "RegexpChunkParser with 1 rules:\n",
      "    Cantidad   <ChunkRule: '[0-9]+|unas|una|un|unos'>\n"
     ]
    }
   ],
   "source": [
    "# Una vez entrenado procedemos a analizar las frases de nuestro corpus mediante un \n",
    "# Regexp parser para obtener las entidades que aparezcan en los textos\n",
    "# Definimos la gramatica que nos permita reconocer comidas y cantidades\n",
    "  # particionar comidas\n",
    "grammar = r\"\"\"\n",
    "  Comida: {<bocadillo|bocata|pizza|piza|pincho|racion|plato>} # Comida\n",
    "  Ingredientes: {(jamon|chorizo|queso|calamares|pimientos|patatas|tortilla)} # Ingredientes\n",
    "  Cantidad: {[0-9]+|unas|una|un|unos} # Cantidad\n",
    "  \"\"\"\n",
    "regex_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "print(regex_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BracketParseCorpusReader' object has no attribute 'markdown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-3922f7c622ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcess_esp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcess_esp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BracketParseCorpusReader' object has no attribute 'markdown'"
     ]
    }
   ],
   "source": [
    "cess_esp.tagged_words()\n",
    "cess_esp.markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGGER HMMs: [('un', 'di0ms0'), ('jamon', 'ncms000')]\n",
      "TAGGER Bigram: [('un', 'di0ms0'), ('jamon', 'NLTK_FASHION')]\n",
      "[('un', 'JJ'), ('jamon', 'NN')]\n",
      "# Input:\n",
      " <JJ>  <NN> \n",
      "# Comida:\n",
      " <JJ>  <NN> \n",
      "# Input:\n",
      " <JJ>  <NN> \n",
      "# Ingredientes:\n",
      " <JJ>  <NN> \n",
      "# Input:\n",
      " <JJ>  <NN> \n",
      "# Cantidad:\n",
      " <JJ>  <NN> \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [('un', 'JJ'), ('jamon', 'NN')])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import Tree\n",
    "from nltk.chunk import *\n",
    "\n",
    "sentencia = \"un jamon\"\n",
    "tokens = nltk.word_tokenize(sentencia)\n",
    "tagged = hmm_tagger.tag(tokens)\n",
    "print (\"TAGGER HMMs:\",tagged)\n",
    "tagged_bigram = bigram_tagger.tag(tokens)\n",
    "print (\"TAGGER Bigram:\",tagged_bigram)\n",
    "prueba = nltk.pos_tag(tokens)\n",
    "print(prueba)\n",
    "# chunked_text = tagstr2tree('quiero/OT un/Cantidad jamon/Comida')\n",
    "# print (chunked_text)\n",
    "# PROBANDO!!!!\n",
    "regex_parser.parse(prueba, trace=True)\n",
    "# regex_parser.parse(tagged_bigram, trace=True)\n",
    "# unchunk_text = chunked_text.flatten()\n",
    "# print (unchunk_text)\n",
    "# regex_parser.parse(chunked_text, trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed_sentence= (S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "  \"\"\"\n",
    "  Natural Language Toolkit: code_cascaded_chunker\n",
    "  http://www.nltk.org/book/ch07.html#code-cascaded-chunker\n",
    "  \"\"\"\n",
    "  grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "  cp = nltk.RegexpParser(grammar)\n",
    "  sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),  (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "  parsed_sentence = cp.parse(sentence)\n",
    "  print('parsed_sentence=', parsed_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
