{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA 1 - NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESOLUCIÓN DEL CASO PRÁCTICO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instalamos los datos (download() solo debe ejecutarse una sola vez si aún no se ha hecho, y a continuación hay que comentar la línea para no volver a descargar todo de nuevo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "#En la ventana que aparece señalamos 'all' para descargar todos los paquetes NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Creamos una texto de entrada a nuestra cadena NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Texto: I didn't notice my animals were uglier than yours! I'm sorry...\n"
     ]
    }
   ],
   "source": [
    "text = \"I didn't notice my animals were uglier than yours! I'm sorry...\"\n",
    "print (\"\\n\\n1. Texto:\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Dividimos el texto en frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. Frases: [\"I didn't notice my animals were uglier than yours!\", \"I'm sorry...\"]\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "print (\"\\n\\n2. Frases:\",sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Tokenización: tokenizamos el texto, es decir dividimos el texto en tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3. Tokens: ['I', 'did', \"n't\", 'notice', 'my', 'animals', 'were', 'uglier', 'than', 'yours', '!', 'I', \"'m\", 'sorry', '...']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "print (\"\\n\\n3. Tokens:\",tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Análisis morfológico: asignamos una etiqueta morfologica a cada token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "4. Analisis Morfologico: [('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('notice', 'VB'), ('my', 'PRP$'), ('animals', 'NNS'), ('were', 'VBD'), ('uglier', 'JJR'), ('than', 'IN'), ('yours', 'JJR'), ('!', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('sorry', 'JJ'), ('...', ':')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "print (\"\\n\\n4. Analisis Morfologico:\",tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Stemming: obtenemos la raíz (en inglés 'stem') de cada token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5. Stems: \n",
      "i\n",
      "did\n",
      "n't\n",
      "notic\n",
      "my\n",
      "anim\n",
      "were\n",
      "uglier\n",
      "than\n",
      "your\n",
      "!\n",
      "i\n",
      "'m\n",
      "sorri\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print (\"\\n\\n5. Stems: \")\n",
    "for tok in tokens:\n",
    "    print (stemmer.stem(tok.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Lematización: obtenemos el lema de cada token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "6. Lemas: \n",
      "i\n",
      "do\n",
      "not\n",
      "notice\n",
      "my\n",
      "animal\n",
      "be\n",
      "ugly\n",
      "than\n",
      "yours\n",
      "!\n",
      "i\n",
      "be\n",
      "sorry\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#El lematizador de wordnet solo reconoce 4 etiquetas POS: a (adjetivo), r(adverbio),n (nombre),v(verbo). \n",
    "#Así que debemos hacer una conversión del formato Penn Tree Bank al formato wordnet (ej: NN->n, JJ->a, RB->r, VB->V, ...)\n",
    "from nltk.corpus import wordnet\n",
    "wnTags = {'N':wordnet.NOUN,'J':wordnet.ADJ,'V':wordnet.VERB,'R':wordnet.ADV} \n",
    "print (\"\\n\\n\\n6. Lemas: \")\n",
    "for (tok,tag) in tagged:\n",
    "    #wordnet no contiene las formas abreviadas 'm , 's y  n't así que las introducimos nosotros para que lematice bien\n",
    "    if tok=='\\'m':\n",
    "        tok = 'am'\n",
    "    if tok=='\\'s':\n",
    "        tok = 'is'\n",
    "    if tok=='n\\'t':\n",
    "        tok = 'not'\n",
    "    tag = tag[:1]\n",
    "    lemma = lemmatizer.lemmatize(tok.lower(),wnTags.get(tag,wordnet.NOUN))\n",
    "    if lemma is None: #Si wordnet no contiene la palabra, supondremos que el lema es igual al token\n",
    "       lemma = tok.lower() \n",
    "    print (lemma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Análisis sintáctico: \n",
    "Partimos de una frase de un conocido texto de Groucho Marx, con una clara ambigüedad: \"While hunting in Africa, I shot an elephant in my pijamas. How he got into my pijamas, I don't know.\" ¿Groucho estaba en pijama o el elefante estaba dentro de su pijama?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisis sintactico:\n",
      "\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pijamas))))) \n",
      "\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pijamas)))))) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "groucho = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pijamas'] #\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pijamas'\n",
    "V -> 'shot' | 'did'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "#Generamos un parser sintáctico capaz de reconocer la gramática\n",
    "parser = nltk.ChartParser(grammar)\n",
    "print ('Analisis sintactico:\\n')\n",
    "for tree in parser.parse(groucho):\n",
    "    print(tree,'\\n')\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMENZAMOS EL EJERCICIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importamos los módulos necesarios\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.- Creamos el texto de entrada a nuestra cadena NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTO: I didn't notice my animals were uglier than yours! I'm sorry...\n"
     ]
    }
   ],
   "source": [
    "text = \"I didn't notice my animals were uglier than yours! I'm sorry...\"\n",
    "#Mostramos el resultado\n",
    "print (\"TEXTO:\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.- Dividimos el texto en frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I didn't notice my animals were uglier than yours!\", \"I'm sorry...\"]\n"
     ]
    }
   ],
   "source": [
    "sentences= nltk.tokenize.sent_tokenize (text)\n",
    "#Mostramos el resultado\n",
    "print (sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.- Tokenizamos el texto de cada frase mediante un bucle, extraemos la etiqueta para cada uno de ellos y, finalmente mostramos su lema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS: ['I', 'did', \"n't\", 'notice', 'my', 'animals', 'were', 'uglier', 'than', 'yours', '!']\n",
      "TAGS: [('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('notice', 'VB'), ('my', 'PRP$'), ('animals', 'NNS'), ('were', 'VBD'), ('uglier', 'JJR'), ('than', 'IN'), ('yours', 'JJR'), ('!', '.')]\n",
      "LEMAS: \n",
      "i\n",
      "do\n",
      "not\n",
      "notice\n",
      "my\n",
      "animal\n",
      "be\n",
      "ugly\n",
      "than\n",
      "yours\n",
      "!\n",
      "TOKENS: ['I', \"'m\", 'sorry', '...']\n",
      "TAGS: [('I', 'PRP'), (\"'m\", 'VBP'), ('sorry', 'JJ'), ('...', ':')]\n",
      "LEMAS: \n",
      "i\n",
      "be\n",
      "sorry\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "#Importamos numpy porque luego vamos a trabajar con arrays para extraer en esta estructura los tags\n",
    "import numpy as np\n",
    "#definimos un array para los tag\n",
    "array_POS = []\n",
    "#definimos un array para los lemas\n",
    "array_lema = []\n",
    "       \n",
    "for sentence in sentences:\n",
    "    tokens=nltk.word_tokenize(sentence)\n",
    "    print (\"TOKENS:\", tokens) #Mostramos el resultado\n",
    "    tags=nltk.pos_tag(tokens)\n",
    "    array_POS.append(tags)\n",
    "    print (\"TAGS:\",tags) #Mostramos el resultado\n",
    "    stemmer=PorterStemmer()\n",
    "    lemmatizer=WordNetLemmatizer ()\n",
    "    from nltk.corpus import wordnet\n",
    "    wnTags = {'N':wordnet.NOUN,'J':wordnet.ADJ,'V':wordnet.VERB,'R':wordnet.ADV} \n",
    "    print (\"LEMAS: \") #Mostramos el resultado\n",
    "    for (tok,tag) in tags:\n",
    "        #wordnet no contiene las formas abreviadas 'm  y  n't que aparecen en la frase así que las introducimos para que lematice bien\n",
    "        if tok=='\\'m':\n",
    "            tok = 'am'\n",
    "        if tok=='n\\'t':\n",
    "            tok = 'not'\n",
    "        tag = tag[:1]\n",
    "        #obtenemos el lema llamando a la funcion wordnet.morphy:\n",
    "        lemma = wordnet.morphy(tok.lower(),wnTags.get(tag,wordnet.NOUN))\n",
    "        #Si wordnet no contiene una de las palabras contenidas en el texto a analizar, suponemos que lema = token\n",
    "        if lemma is None: \n",
    "            lemma = tok.lower() \n",
    "        print (lemma)\n",
    "        #guardamos los lemas obtenidos en el array creado a tal efecto\n",
    "        array_lema.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Array de los lemas: ['i', 'do', 'not', 'notice', 'my', 'animal', 'be', 'ugly', 'than', 'yours', '!', 'i', 'be', 'sorry', '...']\n",
      "- Array de los tags: [[('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('notice', 'VB'), ('my', 'PRP$'), ('animals', 'NNS'), ('were', 'VBD'), ('uglier', 'JJR'), ('than', 'IN'), ('yours', 'JJR'), ('!', '.')], [('I', 'PRP'), (\"'m\", 'VBP'), ('sorry', 'JJ'), ('...', ':')]]\n"
     ]
    }
   ],
   "source": [
    "#Representamos el array de los lemas\n",
    "print (\"- Array de los lemas:\",array_lema)\n",
    "#Representamos el array de los tags\n",
    "print (\"- Array de los tags:\",array_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.- Creamos nuestra propia Gramatica Libre de Contexto (CFG) para los lemas obtenidos de la frase a analizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisis sintactico lema:\n",
      "(S\n",
      "  (NP i)\n",
      "  (VP\n",
      "    (Verb do)\n",
      "    (PP\n",
      "      (Adv not)\n",
      "      (Nom notice)\n",
      "      (NP\n",
      "        (PP\n",
      "          (Pron my)\n",
      "          (Nom animal)\n",
      "          (VP (Verb be) (Adj ugly) (Conj than) (Pron yours))\n",
      "          (Punt ! (NP i (Verb be) (Adj sorry) (Punt ...)))))))) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generamos la gramática para los lemas\n",
    "grammar_lemas = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> Adv Nom NP | Pron Nom VP Punt\n",
    "NP -> 'i' | PP | 'i' Verb Adj Punt\n",
    "VP -> Verb PP | Verb Adj Conj Pron\n",
    "Punt -> '!' NP | '...'\n",
    "Adv ->  'not' \n",
    "Conj -> 'than' \n",
    "Nom -> 'notice' | 'animal'\n",
    "Pron -> 'my' | 'yours' \n",
    "Verb -> 'be' | 'do'\n",
    "Adj -> 'ugly' | 'sorry'\n",
    "\"\"\")\n",
    "\n",
    "#Enviamos los lemas a analizar en la gramática\n",
    "parser = nltk.ChartParser(grammar_lemas)\n",
    "print ('Analisis sintactico lema:')\n",
    "for treelema in parser.parse(array_lema):\n",
    "    print(treelema,'\\n')\n",
    "    treelema.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.- Creamos nuestra propia Gramatica Libre de Contexto (CFG) para los POS obtenidos de la frase a analizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisis sintactico POS:\n",
      "(S\n",
      "  (NP NNS VBP RB)\n",
      "  (VP\n",
      "    VB\n",
      "    PRP$\n",
      "    JJ\n",
      "    VB\n",
      "    (NP RB IN UH (Punt . (NP NN (VP VB JJ (Punt :))))))) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generamos la gramática para los tags utilizando el array creado para los tags\n",
    "nltk.pos_tag(array_lema)\n",
    "grammar_POS = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> 'NNS' 'VBP' 'RB' | 'RB' 'IN' 'UH' Punt |'NN' VP\n",
    "VP -> 'VB' 'PRP$' 'JJ' 'VB' NP | 'VB' 'JJ' Punt\n",
    "Punt -> '.' NP| ':'\n",
    "\"\"\")\n",
    "#Cargo los tags mediante una sentencia (no he logrado hacerlo en este tiempo con el array que había generado antes).\n",
    "sentence_POS = \"NNS VBP RB VB PRP$ JJ VB RB IN UH . NN VB JJ :\".split(\" \") \n",
    "#Creamos el parser\n",
    "parser_POS = nltk.ChartParser(grammar_POS)\n",
    "print ('Analisis sintactico POS:')\n",
    "#Cargamos en el arbol\n",
    "for tree_POS in parser_POS.parse(sentence_POS):\n",
    "    print(tree_POS,'\\n')\n",
    "    tree_POS.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
