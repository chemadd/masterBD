{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Texto: I didn't notice my animals were uglier than yours! I'm sorry...\n",
      "\n",
      "\n",
      "2. Frases: [\"I didn't notice my animals were uglier than yours!\", \"I'm sorry...\"]\n",
      "\n",
      "\n",
      "3. Tokens: ['I', 'did', \"n't\", 'notice', 'my', 'animals', 'were', 'uglier', 'than', 'yours', '!', 'I', \"'m\", 'sorry', '...']\n",
      "\n",
      "\n",
      "4. Analisis Morfologico: [('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('notice', 'VB'), ('my', 'PRP$'), ('animals', 'NNS'), ('were', 'VBD'), ('uglier', 'JJR'), ('than', 'IN'), ('yours', 'JJR'), ('!', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('sorry', 'JJ'), ('...', ':')]\n",
      "\n",
      "\n",
      "5. Stems: \n",
      "i\n",
      "did\n",
      "n't\n",
      "notic\n",
      "my\n",
      "anim\n",
      "were\n",
      "uglier\n",
      "than\n",
      "your\n",
      "!\n",
      "i\n",
      "'m\n",
      "sorri\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "6. Lemas: \n",
      "i\n",
      "do\n",
      "not\n",
      "notice\n",
      "my\n",
      "animal\n",
      "be\n",
      "ugly\n",
      "than\n",
      "yours\n",
      "!\n",
      "i\n",
      "be\n",
      "sorry\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "7. Analisis sintactico:\n",
      "\n",
      "|.  I  . shot.  an .eleph.  in .  my .pijam.|\n",
      "|[-----]     .     .     .     .     .     .| [0:1] 'I'\n",
      "|.     [-----]     .     .     .     .     .| [1:2] 'shot'\n",
      "|.     .     [-----]     .     .     .     .| [2:3] 'an'\n",
      "|.     .     .     [-----]     .     .     .| [3:4] 'elephant'\n",
      "|.     .     .     .     [-----]     .     .| [4:5] 'in'\n",
      "|.     .     .     .     .     [-----]     .| [5:6] 'my'\n",
      "|.     .     .     .     .     .     [-----]| [6:7] 'pijamas'\n",
      "|[-----]     .     .     .     .     .     .| [0:1] NP -> 'I' *\n",
      "|[----->     .     .     .     .     .     .| [0:1] S  -> NP * VP\n",
      "|.     [-----]     .     .     .     .     .| [1:2] V  -> 'shot' *\n",
      "|.     [----->     .     .     .     .     .| [1:2] VP -> V * NP\n",
      "|.     .     [-----]     .     .     .     .| [2:3] Det -> 'an' *\n",
      "|.     .     [----->     .     .     .     .| [2:3] NP -> Det * N\n",
      "|.     .     [----->     .     .     .     .| [2:3] NP -> Det * N PP\n",
      "|.     .     .     [-----]     .     .     .| [3:4] N  -> 'elephant' *\n",
      "|.     .     [-----------]     .     .     .| [2:4] NP -> Det N *\n",
      "|.     .     [----------->     .     .     .| [2:4] NP -> Det N * PP\n",
      "|.     .     [----------->     .     .     .| [2:4] S  -> NP * VP\n",
      "|.     [-----------------]     .     .     .| [1:4] VP -> V NP *\n",
      "|.     [----------------->     .     .     .| [1:4] VP -> VP * PP\n",
      "|[-----------------------]     .     .     .| [0:4] S  -> NP VP *\n",
      "|.     .     .     .     [-----]     .     .| [4:5] P  -> 'in' *\n",
      "|.     .     .     .     [----->     .     .| [4:5] PP -> P * NP\n",
      "|.     .     .     .     .     [-----]     .| [5:6] Det -> 'my' *\n",
      "|.     .     .     .     .     [----->     .| [5:6] NP -> Det * N\n",
      "|.     .     .     .     .     [----->     .| [5:6] NP -> Det * N PP\n",
      "|.     .     .     .     .     .     [-----]| [6:7] N  -> 'pijamas' *\n",
      "|.     .     .     .     .     [-----------]| [5:7] NP -> Det N *\n",
      "|.     .     .     .     .     [----------->| [5:7] NP -> Det N * PP\n",
      "|.     .     .     .     .     [----------->| [5:7] S  -> NP * VP\n",
      "|.     .     .     .     [-----------------]| [4:7] PP -> P NP *\n",
      "|.     .     [-----------------------------]| [2:7] NP -> Det N PP *\n",
      "|.     [-----------------------------------]| [1:7] VP -> VP PP *\n",
      "|.     [----------------------------------->| [1:7] VP -> VP * PP\n",
      "|[=========================================]| [0:7] S  -> NP VP *\n",
      "|.     .     [----------------------------->| [2:7] S  -> NP * VP\n",
      "|.     [-----------------------------------]| [1:7] VP -> V NP *\n",
      "|.     [----------------------------------->| [1:7] VP -> VP * PP\n",
      "|[=========================================]| [0:7] S  -> NP VP *\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pijamas))))) \n",
      "\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pijamas)))))) \n",
      "\n",
      "* Sentence:\n",
      "I saw a dog\n",
      "['I', 'saw', 'a', 'dog']\n",
      "\n",
      "* Strategy: Bottom-up\n",
      "\n",
      "|.    I    .   saw   .    a    .   dog   .|\n",
      "|[---------]         .         .         .| [0:1] 'I'\n",
      "|.         [---------]         .         .| [1:2] 'saw'\n",
      "|.         .         [---------]         .| [2:3] 'a'\n",
      "|.         .         .         [---------]| [3:4] 'dog'\n",
      "|>         .         .         .         .| [0:0] NP -> * 'I'\n",
      "|[---------]         .         .         .| [0:1] NP -> 'I' *\n",
      "|>         .         .         .         .| [0:0] S  -> * NP VP\n",
      "|>         .         .         .         .| [0:0] NP -> * NP PP\n",
      "|[--------->         .         .         .| [0:1] S  -> NP * VP\n",
      "|[--------->         .         .         .| [0:1] NP -> NP * PP\n",
      "|.         >         .         .         .| [1:1] Verb -> * 'saw'\n",
      "|.         [---------]         .         .| [1:2] Verb -> 'saw' *\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb NP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb\n",
      "|.         [--------->         .         .| [1:2] VP -> Verb * NP\n",
      "|.         [---------]         .         .| [1:2] VP -> Verb *\n",
      "|.         >         .         .         .| [1:1] VP -> * VP PP\n",
      "|[-------------------]         .         .| [0:2] S  -> NP VP *\n",
      "|.         [--------->         .         .| [1:2] VP -> VP * PP\n",
      "|.         .         >         .         .| [2:2] Det -> * 'a'\n",
      "|.         .         [---------]         .| [2:3] Det -> 'a' *\n",
      "|.         .         >         .         .| [2:2] NP -> * Det Noun\n",
      "|.         .         [--------->         .| [2:3] NP -> Det * Noun\n",
      "|.         .         .         >         .| [3:3] Noun -> * 'dog'\n",
      "|.         .         .         [---------]| [3:4] Noun -> 'dog' *\n",
      "|.         .         [-------------------]| [2:4] NP -> Det Noun *\n",
      "|.         .         >         .         .| [2:2] S  -> * NP VP\n",
      "|.         .         >         .         .| [2:2] NP -> * NP PP\n",
      "|.         [-----------------------------]| [1:4] VP -> Verb NP *\n",
      "|.         .         [------------------->| [2:4] S  -> NP * VP\n",
      "|.         .         [------------------->| [2:4] NP -> NP * PP\n",
      "|[=======================================]| [0:4] S  -> NP VP *\n",
      "|.         [----------------------------->| [1:4] VP -> VP * PP\n",
      "Nr edges in chart: 33\n",
      "(S (NP I) (VP (Verb saw) (NP (Det a) (Noun dog))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#*************************************************************************\n",
    "#1.Importamos la libreria NLTK\n",
    "#*************************************************************************\n",
    "import nltk\n",
    "\n",
    "\n",
    "#*************************************************************************\n",
    "#2.Creamos una texto de entrada a nuestra cadena NLP\n",
    "#*************************************************************************\n",
    "text = \"I didn't notice my animals were uglier than yours! I'm sorry...\"\n",
    "print (\"\\n\\n1. Texto:\",text)\n",
    "\n",
    "\n",
    "#*************************************************************************\n",
    "#2.Dividimos el texto en frases\n",
    "#*************************************************************************\n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "print (\"\\n\\n2. Frases:\",sentences)\n",
    "\n",
    "\n",
    "#*****************************************************************************\n",
    "#3.Tokenización: tokenizamos el texto, es decir dividimos el texto en tokens\n",
    "#*****************************************************************************\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print (\"\\n\\n3. Tokens:\",tokens)\n",
    "\n",
    "\n",
    "\n",
    "#*************************************************************************\n",
    "#4.Análisis morfológico: asignamos una etiqueta morfologica a cada token\n",
    "#*************************************************************************\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print (\"\\n\\n4. Analisis Morfologico:\",tagged)\n",
    "\n",
    "\n",
    "\n",
    "#*******************************************************************  \n",
    "#5.Stemming: obtenemos la raíz (en inglés 'stem') de cada token\n",
    "#*******************************************************************  \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print (\"\\n\\n5. Stems: \")\n",
    "for tok in tokens:\n",
    "    print (stemmer.stem(tok.lower()))\n",
    "  \n",
    "    \n",
    " \n",
    " \n",
    "#*******************************************************************  \n",
    "#6.Lematización: obtenemos el lema de cada token \n",
    "#*******************************************************************  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#El lematizador de wordnet solo reconoce 4 etiquetas POS: a (adjetivo), r(adverbio),n (nombre),v(verbo). \n",
    "#Así que debemos hacer una conversión del formato Penn Tree Bank al formato wordnet (ej: NN->n, JJ->a, RB->r, VB->V, ...)\n",
    "from nltk.corpus import wordnet\n",
    "wnTags = {'N':wordnet.NOUN,'J':wordnet.ADJ,'V':wordnet.VERB,'R':wordnet.ADV} \n",
    "print (\"\\n\\n\\n6. Lemas: \")\n",
    "for (tok,tag) in tagged:\n",
    "    #wordnet no contiene las formas abreviadas 'm  y  n't así que las introducimos nosotros para que lematice bien\n",
    "    if tok=='\\'m':\n",
    "        tok = 'am'\n",
    "    if tok=='\\'s':\n",
    "        tok = 'is'\n",
    "    if tok=='n\\'t':\n",
    "        tok = 'not'\n",
    "    tag = tag[:1]\n",
    "    lemma = lemmatizer.lemmatize(tok.lower(),wnTags.get(tag,wordnet.NOUN))\n",
    "    #otra forma alternativa de obtener el lema hubiera sido llamar directamente a la funcion wordnet.morphy, que hace lo mismo:\n",
    "    #lemma = wordnet.morphy(tok.lower(),wnTags.get(tag,wordnet.NOUN))\n",
    "    if lemma is None: #Si wordnet no contiene la palabra, supondremos que el lema es igual al token\n",
    "       lemma = tok.lower() \n",
    "    print (lemma)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#*******************************************************************    \n",
    "#7.Análisis sintáctico\n",
    "#******************************************************************* \n",
    "\n",
    "#Partimos de una frase de un conocido texto de Groucho Marx, con una clara ambigüedad: \n",
    "#\"While hunting in Africa, I shot an elephant in my pijamas. How he got into my pijamas, I don't know.\"\n",
    "#¿Groucho estaba en pijama o el elefante estaba dentro de su pijama?\n",
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pijamas']\n",
    "\n",
    "#Creamos nuestra propia Gramatica Libre de Contexto (en inglés CFG)\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pijamas'\n",
    "V -> 'shot' | 'did'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "#Generamos un parser sintáctico capaz de reconocer la gramática\n",
    "parser = nltk.ChartParser(grammar, trace=1)\n",
    "print ('\\n\\n\\n7. Analisis sintactico:\\n')\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree,'\\n')\n",
    "    tree.draw()\n",
    "nltk.parse.chart.demo(2, print_times=False, trace=1, sent='I saw a dog', numparses=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
